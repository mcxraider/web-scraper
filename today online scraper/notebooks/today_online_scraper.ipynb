{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FOR RUNNING IN GOOGLE COLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49CCtGhbEPoP"
      },
      "outputs": [],
      "source": [
        "# You need to import these if u want to run it in colab as a notebook\n",
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "import json\n",
        "from tqdm import trange\n",
        "\n",
        "\n",
        "# Function to navigate to a specific category URL\n",
        "def go_to_category(driver, category_URL):\n",
        "    driver.get(category_URL)\n",
        "def scrape_categories(driver, base_url):\n",
        "    driver.get(base_url)\n",
        "    time.sleep(2)\n",
        "\n",
        "    try:\n",
        "        navbar_items = driver.find_elements(By.CSS_SELECTOR, \"li.main-menu__item a.main-menu__link\")\n",
        "        exclude = ['watch, myfeed']\n",
        "        sub_cats = []\n",
        "        for item in navbar_items:\n",
        "            heading = item.get_attribute(\"href\").split('/')[-1]\n",
        "            if heading not in exclude:\n",
        "                sub_cats.append(heading)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting sub category: {e}\")\n",
        "    sub_cats.remove('watch')\n",
        "    sub_cats.remove('myfeed')\n",
        "    return sub_cats\n",
        "\n",
        "# Function to gather article URLs from a category page\n",
        "def gather_article_urls(driver):\n",
        "    urls = []\n",
        "    article_cards = driver.find_elements(By.CLASS_NAME, \"card-object__figure\")\n",
        "    for card in article_cards:\n",
        "        try:\n",
        "            article_link = card.find_element(By.CLASS_NAME, \"link\")\n",
        "            url = article_link.get_attribute(\"href\")\n",
        "            urls.append(url)\n",
        "        except Exception as e:\n",
        "            print(f\"Error finding article URL: {e}\")\n",
        "    return urls\n",
        "\n",
        "# Function to scrape articles from gathered URLs\n",
        "def scrape_articles(scraped_articles, driver, urls, sub_url):\n",
        "    sub_url_ls = []\n",
        "    for url in urls:\n",
        "        sub_url_hash = {}\n",
        "        sub_url_hash[\"Article URL\"] = url\n",
        "        try:\n",
        "                # Navigate to the article page\n",
        "                driver.get(url)\n",
        "                time.sleep(2)  # Waiting for the page to load\n",
        "\n",
        "                # Get article summaries\n",
        "                summary_elements = driver.find_elements(By.CSS_SELECTOR, \"div.text-long ul li\")\n",
        "                summary_texts = [li.text for li in summary_elements]\n",
        "                Combined_Summary = \".\\n\".join(summary_texts)\n",
        "                if len(Combined_Summary) == 0:\n",
        "                    sub_url_hash[\"Article Summary\"] = 'NO SUMMARY'\n",
        "                else:\n",
        "                    sub_url_hash[\"Article Summary\"] = Combined_Summary\n",
        "\n",
        "                # Get article body\n",
        "                try:\n",
        "                    paragraphs = driver.find_elements(By.CSS_SELECTOR, \"div.text-long p\")\n",
        "                    body = \"\\n\".join([p.text for p in paragraphs])\n",
        "                    sub_url_hash['Article Body'] = body\n",
        "                except Exception as e:\n",
        "                    sub_url_hash['Article Body'] = None\n",
        "                    print(\"Body paragraphs not extracted\")\n",
        "\n",
        "        except Exception as e:\n",
        "                print(f\"Error scraping article: {e}\")\n",
        "        sub_url_ls.append(sub_url_hash)\n",
        "    section = f\"{sub_url} section\"\n",
        "    scraped_articles[section] = sub_url_ls\n",
        "\n",
        "def run_scraper(base_url):\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    options.add_argument('--disable-gpu')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    driver.maximize_window()\n",
        "    scraped_articles = {}\n",
        "    driver.get(base_url)\n",
        "    time.sleep(2)  # Wait for the page to load\n",
        "    category_sub_URLs = scrape_categories(driver, base_url)\n",
        "    for i in trange(len(category_sub_URLs)):\n",
        "        print(f'Collecting data for {category_sub_URLs[i]} section...')\n",
        "        full_category_url = f\"{base_url}{category_sub_URLs[i]}\"\n",
        "        go_to_category(driver, full_category_url)\n",
        "        urls = gather_article_urls(driver)\n",
        "        scrape_articles(scraped_articles, driver, urls, category_sub_URLs[i])\n",
        "        print(f\"\\nDone collecting data for {category_sub_URLs[i]} section...\\n\")\n",
        "    # Close the driver after scraping\n",
        "    driver.quit()\n",
        "    # Write files to json\n",
        "    with open(\"articles.json\", \"w\", encoding='utf-8') as fout:\n",
        "        json.dump(scraped_articles, fout, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AGwht2KEShw"
      },
      "outputs": [],
      "source": [
        "# Uncomment code to run scraper\n",
        "# run_scraper(\"https://www.todayonline.com/\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
